{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "PROJECT'S TITLE\n",
    "\n",
    "DATA SCIENTIST PROJECT RELATED TO THE ANALYSIS OF HOSPITALIZATION DATA \n",
    " IN FRANCE DUE TO COVID-19.\n",
    "\n",
    "\n",
    "PROJET/PURPOSE:\n",
    "\n",
    "\n",
    "The UDACITY DATA SCIENTIST CAPSTONE project consists in building a data\n",
    " science project of my choice with two deliverables:\n",
    " \n",
    " - A Github repository of my work;\n",
    "\n",
    " - A blog post written for technical audience (my choice).\n",
    "\n",
    "\n",
    "Considering the rebound of COVID-19 cases in France these last weeks,\n",
    " I decided to analyze data of COVID-19 progress in France with figures\n",
    " directly reported by the hospitals via an institutional site.\n",
    "\n",
    "1 - I will define the problem I want to solve.\n",
    "  \n",
    "2 - I will analyze this problem through visualization and data exploration\n",
    "  to have a better understanding of the process to implement.\n",
    "\n",
    "3 - I will implement the algorithms and metrics to solve this problem.\n",
    "\n",
    "4 - I will collect my results and conclude to what extend I solve the\n",
    "    problem.\n",
    "\n",
    "5 - I will propose a blog post to document the steps or my work.\n",
    "\n",
    "\n",
    "The notebook here-below gathers my work for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 1 - DEFINITION OF THE PROBLEM TO SOLVE\n",
    "\n",
    "\n",
    "Using data from institutional site, I will focus my attention on the\n",
    " hospitalization, i.e. how COVID-19 fills our hospitals with new\n",
    " patients and the capacity of our hospitals to absorb these new\n",
    " patients.\n",
    "\n",
    "I will monitor the hospitalization in the context of the population and\n",
    " the rate of people positive to the COVID-19 and the mortality. I will\n",
    " work at the scale of the department and even the region. My attention\n",
    " will be particularly focused on my department and my region.\n",
    "\n",
    "The problem to solve consists in providing threshold on parameters to \n",
    " identify that would indicate when increase of hospitalization would\n",
    " significantly increase the risk of going in resuscitation or intensive\n",
    " care, or even the risk of death.\n",
    "\n",
    "In that perspective, I will also try to propose a model for predicting\n",
    " the number of hospitalization. ............................................................................. TBC\n",
    "\n",
    "For measuring efficiency of my solution, I propose following metrics:\n",
    "............................................................................................................. TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# import libraries\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "# import pygal\n",
    "# from pygal_maps_fr import maps\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# requires installing\n",
    "# https://gtk-win.sourceforge.io/home/index.php/Main/Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - Data Exploration\n",
    "\n",
    "\n",
    "I've gathered several data files reporting various information related\n",
    " to the epidemic of COVID-19 in France from the institutional web site:\n",
    " https://www.data.gouv.fr/fr/datasets\n",
    "\n",
    "The source web site indicates that data have been gathered along the\n",
    " time from various formats and contents.\n",
    "\n",
    "All the files used are under the License \"Open Data Commons Open\n",
    " Database License (ODbL)\". \n",
    "\n",
    "I downloaded the following data files the 29 December 2022. All data are\n",
    " stored in Github of this project.\n",
    "\n",
    "\n",
    "FILE # 1\n",
    "\n",
    "- Initial name: \"donnees-vaccination-par-tranche-dage-type-de-vaccin-et-departement.csv\"\n",
    "\n",
    "- New name: \"COVID-19_France_data_vaccin_population.csv\"\n",
    "\n",
    "- Steady URL: \"https://www.data.gouv.fr/fr/datasets/donnees-vaccination-par-tranche-dage-type-de-vaccin-et-departement-region/\"\n",
    "\n",
    "- Last update: 13-Dec-2022\n",
    "\n",
    "- License ID: 60bdce49abcc8f5dcb2fcb3b\n",
    "\n",
    "- Brief description: Vaccination data by age bracket, type of vaccine\n",
    "                     and department / region.\n",
    "\n",
    "- Size: 146 Mb\n",
    "\n",
    "\n",
    "FILE #2\n",
    "\n",
    "- Initial name: \"donnees-vaccination-par-pathologie.csv\"\n",
    "\n",
    "- New name: \"COVID-19_France_data_vaccin_pathology.csv\"\n",
    "\n",
    "- Steady URL: \"https://www.data.gouv.fr/fr/datasets/donnees-vaccination-par-pathologie-et-departement-region/\"\n",
    "\n",
    "- Last update: 13-Dec-2022\n",
    "\n",
    "- License ID: 60bdce48e306a187d32ee2c1\n",
    "\n",
    "- Brief description: Vaccination data per pathology and department\n",
    "                     / region.\n",
    "\n",
    "- Size: 1.2 Mb\n",
    "\n",
    "\n",
    "FILE #3\n",
    "\n",
    "- Initial name: \"table-indicateurs-open-data-dep-2022-12-28-19h00.csv\"\n",
    "\n",
    "- New name: \"COVID-19_France_data_epidemic_indicators.csv\"\n",
    "\n",
    "- Steady URL: \"https://www.data.gouv.fr/fr/datasets/synthese-des-indicateurs-de-suivi-de-lepidemie-covid-19/\"\n",
    "\n",
    "- Last update: 28-Dec-2022\n",
    "\n",
    "- License ID: 60190d00a7273a8100dd4d38\n",
    "\n",
    "- Brief description: COVID-19 epidemic monitoring indicators summary\n",
    "\n",
    "- Size: 11.2 Mb\n",
    "\n",
    "\n",
    "FILE #4\n",
    "\n",
    "- Initial name: \"vacsi-dep-2022-12-28-19h00.csv\"\n",
    "\n",
    "- New name: \"COVID-19_France_data_vaccin_indicators.csv\"\n",
    "\n",
    "- Steady URL: \"https://www.data.gouv.fr/fr/datasets/r/735b0df8-51b4-4dd2-8a2d-8e46d77d60d8\"\n",
    "\n",
    "- Last update: 28-Dec-2022\n",
    "\n",
    "- License ID: 6010206e7aa742eb447930f7\n",
    "\n",
    "- Brief description: Data related to people vaccinated against COVID-19\n",
    "                   per department.\n",
    "\n",
    "- Size: 6.2 Mb\n",
    "\n",
    "\n",
    "FILE #5\n",
    "\n",
    "- Initial name: \"vacsi-tot-dep-2022-12-28-19h00.csv\"\n",
    "\n",
    "- New name: \"COVID-19_France_data_vaccin_indicators_tot.csv\"\n",
    "\n",
    "- Steady URL: \"https://www.data.gouv.fr/fr/datasets/r/7969c06d-848e-40cf-9c3c-21b5bd5a874b\"\n",
    "\n",
    "- Last update: 28-Dec-2022\n",
    "\n",
    "- License ID: 6010206e7aa742eb447930f7\n",
    "\n",
    "- Brief description: Data related to cumulative people vaccinated against\n",
    "                   COVID-19 per department.\n",
    "\n",
    "- Size: 10 Kb\n",
    "\n",
    "\n",
    "COMMENT:\n",
    "\n",
    "Data file were renamed to avoid being dependent of the date that may be\n",
    " notified in the file's name and to simplify and gather the designation\n",
    " of these files in the workspace.\n",
    "\n",
    "\n",
    "All these files are known to be of type 'txt/csv'. They are fulfilled \n",
    " in french such that I was required to open these files under Unicode\n",
    " UTF-8 format.\n",
    "\n",
    "\n",
    "Lets explore these files through the notebook!\n",
    "\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS\n",
    "\n",
    "# Purpose: Define the main directory of the data files: 'work_dir'\n",
    "# Instruction: Modify value of 'work_dir' according to your workspace.\n",
    "\n",
    "dir_1 = r\"C:\\Users\\to202835\\OneDrive - ATR\\_exploitation\\formation\"\n",
    "dir_2 = r\"\\2022\\Udacity_DataScience\\06_DataScientist Capstone\\v1.3.0\"\n",
    "work_dir = dir_1 + dir_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS\n",
    "\n",
    "# Purpose: Build a function that read input csv files\n",
    "# Discussion: Though some of the used data files are also available in\n",
    "#             json format, I prefered get all my data files in the same\n",
    "#             format to ease the read.\n",
    "\n",
    "def read_csv(filename,\n",
    "             dtype_values={},\n",
    "             filedir=work_dir,\n",
    "             separator=',',\n",
    "             encod_errors='ignore'):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Read input csv files\n",
    "    INPUT\n",
    "        filename (string) is the name of the data file to read;\n",
    "        dtype_values (dictionary) is the set of information to enforce\n",
    "                     read of selected columns in a given format to \n",
    "                     possibly avoid warning message of Low memory:\n",
    "                     default = none;\n",
    "        filedir=work_dir (string) is the path of the data files \n",
    "                         directory.\n",
    "                         default = work_dir (previously defined)\n",
    "    OUTPUT\n",
    "        df (pandas dataframe) is the dataframe resulting from the read\n",
    "           of the input data file. If the read fails, it returns an \n",
    "           empty dataframe.\n",
    "    '''\n",
    "\n",
    "    # Build the complete file path\n",
    "    filepath = \"\\\\\".join((filedir, filename))\n",
    "\n",
    "    # trying read the csv data file\n",
    "    try:\n",
    "        df = pd.read_csv(filepath.replace('\\\\','/'),\n",
    "             dtype=dtype_values, sep=separator,\n",
    "             encoding_errors=encod_errors,\n",
    "             low_memory=False)\n",
    "        print(\"- Opening '{}'\".format(filename))\n",
    "    except:\n",
    "        df = pd.DataFrame([])\n",
    "        print(\"- Unable to open '{}'\".format(filename))\n",
    "        raise\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS - FILE #1\n",
    "\n",
    "# define input features\n",
    "f_file_1 = r\"COVID-19_France_data_vaccin_population.csv\"\n",
    "'''Discussion\n",
    "Initial read of the file report an issue on format of column #2 that \n",
    " causes a low memory warning. The issue was solved by setting the\n",
    " appropriate format of some columns with mixed types (identified after\n",
    " the initial opening and analysis of the file) and by using separator\n",
    " \";\".\n",
    "'''\n",
    "dtypes = {'date_reference': str,\n",
    " 'semaine_injection': str,\n",
    " 'region_residence': str,\n",
    " 'libelle_region': str,\n",
    " 'departement_residence': str,\n",
    " 'libelle_departement': str,\n",
    " 'classe_age': str,\n",
    " 'libelle_classe_age': str,\n",
    " 'type_vaccin': str,\n",
    " 'date': str}  # These columns have a mixed type of data\n",
    "\n",
    "# Read the file\n",
    "df_file_1 = read_csv(f_file_1, separator=\";\")\n",
    "# Get a view on this file\n",
    "df_file_1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS - FILE #2\n",
    "\n",
    "# define input features\n",
    "f_file_2 = r\"COVID-19_France_data_vaccin_pathology.csv\"\n",
    "# Read the file\n",
    "df_file_2 = read_csv(f_file_2, separator=\";\")\n",
    "# Get a view on this file\n",
    "df_file_2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS - FILE #3\n",
    "\n",
    "# define input features\n",
    "f_file_3 = r\"COVID-19_France_data_epidemic_indicators.csv\"\n",
    "'''Discussion\n",
    "Initial read of the file report an issue on format of column #2 that \n",
    " causes a low memory warning. The issue was solved by setting the\n",
    " appropriate format of some columns with mixed types (identified after\n",
    " the initial opening and analysis of the file).\n",
    "'''\n",
    "dtypes = {\"date\": str,\n",
    "          \"dep\": str,\n",
    "          \"lib_dep\": str,\n",
    "          \"lib_reg\": str}  # These columns have a mixed type of data\n",
    "\n",
    "# Read the file\n",
    "df_file_3 = read_csv(f_file_3, dtype_values=dtypes)\n",
    "# Get a view on this file\n",
    "df_file_3.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS - FILE #4\n",
    "\n",
    "# define input features\n",
    "f_file_4 = r\"COVID-19_France_data_vaccin_indicators.csv\"\n",
    "dtypes = {\"dep\": str}  # column 'dep' has mixed types. So I enforce the\n",
    "                       #  format while reading the csv file to avoid a\n",
    "                       #  low memory warning message. \n",
    "# Read the file\n",
    "df_file_4 = read_csv(f_file_4,\n",
    "                         dtype_values=dtypes,\n",
    "                         separator=\";\")\n",
    "# Get a view on this file\n",
    "df_file_4.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - DATA ACCESS - FILE #5\n",
    "\n",
    "# define input features\n",
    "f_file_5 = r\"COVID-19_France_data_vaccin_indicators_tot.csv\"\n",
    "dtypes = {\"dep\": str}  # column 'dep' has mixed types. So I enforce the\n",
    "                       #  format while reading the csv file to avoid a\n",
    "                       #  low memory warning message. \n",
    "# Read the file\n",
    "df_file_5 = read_csv(f_file_5,\n",
    "                         dtype_values=dtypes,\n",
    "                         separator=\";\")\n",
    "# Get a view on this file\n",
    "df_file_5.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA\n",
    "\n",
    "\n",
    "Before going deeper in the understanding and definition of every \n",
    " parameters of these files, I'd like to have a view on the type of data\n",
    " and availability of valid data.\n",
    "In addition, in the perspective of merging these files, I will try\n",
    " finding some links that would allow me benefit from a large quantity of \n",
    " data for pursuing the analysis and solve my problem.\n",
    "\n",
    "I'm starting by building a function that would allows me checking some \n",
    " charactertics of the data set per column.\n",
    "It will help me identifying necessary processing of the raw data and\n",
    " checking that I produce relative appropriate data (format and value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA\n",
    "\n",
    "# Build a function to analysis the type and availability of valid data.\n",
    "\n",
    "def display_columns_info(df):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Display some information per column related to the data set\n",
    "    INPUT\n",
    "        df is the dataframe\n",
    "    OUTPUT\n",
    "        nil (only display)    \n",
    "    '''\n",
    "    \n",
    "    # Measure length of the column's title only for presentation purpose\n",
    "    max_len = 0\n",
    "    cols = np.array(df.columns)\n",
    "    for c in cols:\n",
    "        if len(c) > max_len:\n",
    "            max_len = len(c)\n",
    "    size = max_len + 2  # I will create a regulat length for label field.\n",
    "\n",
    "    # Display information per column        \n",
    "    field_1 = \"column label\" + ' '*(size - 11)\n",
    "    field_size = 10\n",
    "    field_2 = \"      size \"\n",
    "    field_3 = \"      type \"\n",
    "    field_4 = \"   nb uniq  rate uniq \"\n",
    "    field_5 = \"    nb nan   rate nan \"\n",
    "    field_6 = \"    nb inf   rate inf \"\n",
    "    field_7 = \"     other \"\n",
    "    # Introducing the results\n",
    "    m = field_1 + field_2 + field_3 + field_4 + field_5 + field_6 + field_7\n",
    "    print(m)\n",
    "\n",
    "    # Then run along the columns to perform and give the analysis per column.\n",
    "    for col in cols:\n",
    "        # column's size\n",
    "        col_size = df[col].shape[0]\n",
    "        # get column's type        \n",
    "        col_type = df[col].dtypes\n",
    "        cot_typ = str(col_type)\n",
    "        # get number and rate of unique values\n",
    "        values_uniq = np.array(pd.unique(df[col]))\n",
    "        nb_uniq = len(values_uniq)\n",
    "        rate_uniq = 100 * nb_uniq / col_size\n",
    "        # get number and rate of nan values\n",
    "        nb_nan = df[col].isna().sum()\n",
    "        rate_nan = 100 * nb_nan / col_size\n",
    "        # get number and rate of infinite values\n",
    "        nb_inf = 0\n",
    "        mixed_types = []\n",
    "        # Run along values of the column\n",
    "        for value in np.array(df[col].values):\n",
    "            # Count infinite value\n",
    "            if (value == np.inf) or (value == -np.inf):\n",
    "                nb_inf += 1\n",
    "            # Check consistency of the value type with its column\n",
    "            if \" \" in str(type(value)):\n",
    "                val_type = str(type(value)).split(\"'\")[1]\n",
    "            else:\n",
    "                if \".\" in str(type(value)):\n",
    "                    val_type = str(type(value)).split(\".\")[1][:-1]\n",
    "                else:\n",
    "                    val_type = str(type(value))\n",
    "            if (val_type != cot_typ):\n",
    "                if val_type not in mixed_types:\n",
    "                    mixed_types.append(val_type)\n",
    "\n",
    "        # Compute the rate of infinite values in the column\n",
    "        rate_inf = 100 * nb_inf / col_size\n",
    "\n",
    "        # Report mixed types if any\n",
    "        msg_mixed_types = ''\n",
    "        if len(mixed_types) > 1:\n",
    "            msg_mixed_types = '\\tMixed types: ' + str(mixed_types)\n",
    "\n",
    "        # Check among 'date' values that they are all strings with a length\n",
    "        #  of 10.\n",
    "        mem_unexp_dates = []\n",
    "        msg_unexp_dates = ''\n",
    "        if col == 'date':\n",
    "            for value in values_uniq:\n",
    "                if (not isinstance(value, str)) or (len(value) != 10):\n",
    "                    mem_unexp_dates.append(value)\n",
    "            if len(mem_unexp_dates) > 0:\n",
    "                msg_unexp_dates = '\\tWrong date format: ' + str(mem_unexp_dates)\n",
    "\n",
    "        # Display result\n",
    "        dec = 2\n",
    "        print('{}{} {}{} {}{} {}{}{}{} % {}{}{}{} % {}{}{}{} % {} {}'.format(\n",
    "            col, ' '*(size - len(col)),\n",
    "            ' ' * (field_size - len(str(col_size))), col_size,\n",
    "            ' ' * (field_size - len(str(col_type))), col_type,\n",
    "            ' ' * (field_size - len(str(nb_uniq))), nb_uniq,\n",
    "            ' ' * (field_size -1 - len(str(round(rate_uniq, dec)))), round(rate_uniq, dec),\n",
    "            ' ' * (field_size - len(str(nb_nan))), nb_nan,\n",
    "            ' ' * (field_size -1 - len(str(round(rate_nan, dec)))), round(rate_nan, dec),\n",
    "            ' ' * (field_size - len(str(nb_inf))), nb_inf,\n",
    "            ' ' * (field_size -1 - len(str(round(rate_inf, dec)))), round(rate_inf, dec),\n",
    "            msg_unexp_dates,\n",
    "            msg_mixed_types)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #1\n",
    "\n",
    "# Check the data\n",
    "display_columns_info(df_file_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #1\n",
    "\n",
    "\n",
    "Result of the check of the dataframe 'df_file_1' reveals that:\n",
    "\n",
    "- 'date_reference', 'semaine_injection', 'region_residence', \n",
    "  'libelle_region', 'departement_residence', 'libelle_departement',\n",
    "  'classe_age', 'libelle_classe_age', 'type_vaccin' and 'date' are\n",
    "  defined as 'object'.\n",
    "\n",
    "  ACTION: Think to convert these columns in dummies if needed for \n",
    "   modeling or other use.\n",
    "\n",
    "- 'population_insee' contains 1.64% nan values.\n",
    "\n",
    "   ACTION: Monitor the need to keep these rows or replace nan!\n",
    "\n",
    "- 'effectif_cumu_1_inj', 'effectif_cumu_termine', 'taux_cumu_1_inj' and\n",
    "  'taux_cumu_termine' contain around 50% nan values.\n",
    "\n",
    "   ACTION: Monitor the need to keep these rows or replace nan if\n",
    "    possible!\n",
    "\n",
    "- 'effectif_1_inj', 'effectif_termine', 'taux_1_inj', 'taux_termine',\n",
    "  'effectif_rappel', 'effectif_cumu_rappel', \n",
    "  'effectif_rappel_parmi_eligible', 'effectif_eligible_au_rappel',\n",
    "  'taux_rappel', 'taux_cumu_rappel' and 'taux_cumu_rappeleli'\n",
    "  contain more than 70% nan values; I would recommend removing \n",
    "  this column because almost the whole column has no consistent \n",
    "  values.\n",
    "  \n",
    "  ACTION: Remove this column immediately\n",
    "\n",
    "\n",
    "Possible improvement:\n",
    "Automatize cleaning according the the level of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #1 - CLEANING\n",
    "\n",
    "# Remove columns almost empty (at least 50% of nan):\n",
    "list1 = ['effectif_cumu_1_inj', 'effectif_cumu_termine',\n",
    "         'taux_cumu_1_inj', 'taux_cumu_termine', 'effectif_1_inj',\n",
    "         'effectif_termine', 'taux_1_inj', 'taux_termine', \n",
    "         'effectif_rappel', 'effectif_cumu_rappel', \n",
    "         'effectif_rappel_parmi_eligible', 'effectif_eligible_au_rappel',\n",
    "          'taux_rappel', 'taux_cumu_rappel', 'taux_cumu_rappeleli']\n",
    "df_file_1.drop(columns=list1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #2\n",
    "\n",
    "# Check the data\n",
    "display_columns_info(df_file_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #2\n",
    "\n",
    "\n",
    "Result of the check of the dataframe 'df_file_2' reveals that:\n",
    "\n",
    "- 'date', 'regroupement_pathologie', 'pathologie', 'region_residence',\n",
    "  'libelle_region', 'departement_residence' and 'libelle_departement'\n",
    "   are defined as 'object'.\n",
    "   \n",
    "   ACTION: Think to convert these columns in dummies if needed for \n",
    "    modeling or other use.\n",
    "\n",
    "- 'libelle_region' contains 0.85% nan values.\n",
    "\n",
    "- 'population_patho_cartographie' contains 2.22% nan values.\n",
    "\n",
    "- 'effectif_1_inj_pathologie', 'effectif_termine_pathologie', \n",
    "  'taux_1_inj_pathologie', 'taux_termine_pathologie' and\n",
    "  'effectif_eligible_rappel_patho' contain 2.42% nan values.\n",
    "\n",
    "- 'effectif_rappel_parmi_eli_patho', 'taux_rappel_pathologie' and\n",
    "  'taux_rappel_eligible_pathologie' contain 3.15%nan values.\n",
    "\n",
    "- 'ordre' contains 12.28% nan values.\n",
    "\n",
    "ACTION: Monitor the need to keep these rows with nan or replace nan!\n",
    "\n",
    "\n",
    "=> No immediate action required! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #3\n",
    "\n",
    "# Check the data\n",
    "display_columns_info(df_file_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #3\n",
    "\n",
    "\n",
    "Result of the check of the dataframe 'df_file_2' reveals that:\n",
    "\n",
    "- Although enforcing format of 'dep', 'date', 'lib_dep' and 'lib_reg'\n",
    "   into str while reading the csv file, type of these columns is still\n",
    "   'object'.\n",
    "\n",
    "   ACTION: Think to convert these columns in dummies if needed for \n",
    "    modeling or other use.\n",
    "\n",
    "- 'incid_hosp', 'incid_rea', 'incid_dchosp' and 'reg_incid_rea' contain\n",
    "   0.1% nan values.\n",
    "\n",
    "   ACTION: Monitor the need to keep these rows or replace nan!\n",
    "\n",
    "- 'tx_pos', 'tx_incid', 'pos' and 'pos_7j' contain 5.81% of nan values.\n",
    "\n",
    "   ACTION: Monitor the need to keep these rows or replace nan!\n",
    "\n",
    "- 'R' contains 84.94% nan values; I would recommend removing this\n",
    "   column because too many values are missing without possibility to \n",
    "   replace missing values with a relevant value.\n",
    "\n",
    "   ACTION: remove this column immediately.\n",
    "\n",
    "- 'cv_dose1' contains 99.9% nan values; I would recommend removing this\n",
    "   column because almost the whole column has no consistent values.\n",
    "   \n",
    "   ACTION: remove this column immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #3 - CLEANING\n",
    "\n",
    "# Remove columns with a lot of missing data: 'R' and 'cv_dose1'.\n",
    "list3 = ['R', 'cv_dose1']\n",
    "df_file_3.drop(columns=list3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #4\n",
    "\n",
    "# Check the data\n",
    "display_columns_info(df_file_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #4\n",
    "\n",
    "\n",
    "Result of the check of the dataframe 'df_file_4' reveals that:\n",
    "\n",
    "- 'dep' and 'jour' are defined as 'object' while I've enforced 'dep' \n",
    "  into string format.\n",
    "\n",
    "  ACTION: Think to convert these columns in dummies if needed for \n",
    "    modeling or other use.\n",
    "\n",
    "- There is neither nan nor infinite values in this dataframe.\n",
    "\n",
    "  ACTION: nil\n",
    "\n",
    "- To be homogeneous with other dataframes, 'jour' must be replaced by\n",
    "  'date'.\n",
    "  \n",
    "  ACTION: replace column's label 'jour' by 'date' (same format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #4 - CLEANING\n",
    "\n",
    "# Rename column 'jour' as 'date'\n",
    "df_file_4.rename({'jour': 'date'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #5\n",
    "\n",
    "# Check the data\n",
    "display_columns_info(df_file_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #5\n",
    "\n",
    "\n",
    "Result of the check of the dataframe 'df_file_5' reveals same comments\n",
    "that for 'df_file_4'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - TYPE AND VALIDITY OF DATA - FILE #5 - CLEANING\n",
    "\n",
    "# Rename column 'jour' as 'date'\n",
    "df_file_5.rename({'jour': 'date'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "SECTION 2 - ANALYSIS - EXPLORATION AND VISUALIZATION\n",
    "\n",
    "\n",
    "AT this level, I'd like to merge all dataframes to benefit the maximum\n",
    " data to work on later. But is it possible?\n",
    "My main concern, since I use data file with time series, is that they \n",
    " all covers the common time period long enought to keep a lot of data \n",
    " to work on.\n",
    "\n",
    "Let's have a view on the time series on every input file.\n",
    "\n",
    "Once we would have identified the input data files that I can keep or\n",
    " no, I will merge them by the columns 'date' and 'dep'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - ANALYSIS - EXPLORATION - FILES MERGING\n",
    "\n",
    "# Basically, I want to know how many unique dates owns every dataframe:\n",
    "print('Number of unique values of date per input data file:')\n",
    "print(\"File #1:\", len(np.unique(np.array(df_file_1['date']))))\n",
    "print(\"File #2:\", len(np.unique(np.array(df_file_2['date']))))\n",
    "print(\"File #3:\", len(np.unique(np.array(df_file_3['date']))))\n",
    "print(\"File #4:\", len(np.unique(np.array(df_file_4['date']))))\n",
    "print(\"File #5:\", len(np.unique(np.array(df_file_5['date']))))\n",
    "# note: Applying the function 'display_columns_info' give me already\n",
    "#  the same result but it allows to summarize it here.\n",
    "\n",
    "# Result: I see that merging all files would give me only one date.\n",
    "#         So I need to identify clearly the covering time period of\n",
    "#         every files to see which ones I could merge.\n",
    "#         A priori, I would keep only files 3 and 4, let's check that.\n",
    "#         Files 2 and 5 return only one date: ['2022-12-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - DATA VISUALIZATION - PLOTTING TIME SERIES\n",
    "\n",
    "# Build a generic function for displaying time series\n",
    "\n",
    "def plot_time_series(df, label: str, departments: list):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Plot a line chart of figures for the given label and the \n",
    "         selected list of departments.\n",
    "    INPUT\n",
    "        df is the pandas dataframe;\n",
    "        label (string) is the name of the column for which I want a\n",
    "              plot;\n",
    "        departments (list) is the list departments for which I want\n",
    "                    a plot.\n",
    "    OUTPUT\n",
    "        nil (only display)\n",
    "    '''\n",
    "\n",
    "    # If not already done, I group df by department and timestamp\n",
    "    # df.groupby(['dep', 'timestamp'])  # If no dummy applied\n",
    "    df.groupby(['dep', 'date'])  # If no dummy applied\n",
    "\n",
    "    # get the uniq list of the selected departments, just in case of...\n",
    "    departments = np.unique(departments)  # just in case of \n",
    "    \n",
    "    df_plot_list = []  # list to store data for plotting of every\n",
    "                       #  selected departments.\n",
    "    # Run along the list of departments\n",
    "    for dep in departments:\n",
    "        # Get the values of the label along the time\n",
    "        df_dep = pd.DataFrame([])\n",
    "        df_dep = df[df['dep'] == dep]\n",
    "\n",
    "        # Get the time for this dep:\n",
    "        x_val = np.array(df_dep['date'])\n",
    "        \n",
    "        # get the values of the label for this dep\n",
    "        y_val = np.array(df_dep[label])\n",
    "\n",
    "        # Create a dataframe for the plot\n",
    "        df_plot = pd.DataFrame(dict(x = x_val, y = y_val))\n",
    "\n",
    "        # store the data for plotting\n",
    "        df_plot_list.append(df_plot)\n",
    "\n",
    "    # Plotting\n",
    "    # declare the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Run along the list of dataframes containing plotting data\n",
    "    for i, df_plot in enumerate(df_plot_list):\n",
    "        # Add a line of result per dataframe\n",
    "        fig.add_trace(go.Scatter(x=np.array(df_plot['x']),\n",
    "                                y=np.array(df_plot['y']),\n",
    "                                mode='lines+markers',\n",
    "                                name='Dept. ' + str(departments[i])\n",
    "                                )\n",
    "                    )\n",
    "    # Define the context of the plot\n",
    "    titre = df_dict_labels[label] + \"\\n(per department)\"\n",
    "    fig.update_layout(title=titre,\n",
    "                    xaxis_title=\"Time\",\n",
    "                    yaxis_title=df_dict_labels[label],\n",
    "                    legend=dict(traceorder='reversed'))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - DATA VISUALIZATION - PLOTTING TIME SERIES\n",
    "\n",
    "# I build a function to format x/y values for plotting lines\n",
    "\n",
    "def get_df_dates(dfs, label='timestamp'):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        return dataframes with min/max of the column label in abscisse\n",
    "         and index in ordonate. For visualization purpose\n",
    "    INPUT\n",
    "        dfs (list) is a list of dataframes where I will search for values \n",
    "            in the column named label;\n",
    "        label (string) is the name of the column where I will lokk for \n",
    "              timestamp values; default = 'timestamp'.\n",
    "    OUTPUT\n",
    "        dfs_return (list) is a list of dataframes such that every\n",
    "                   dataframe is defined like:\n",
    "                   {'x' = [values of ‘dates’ from the dataframe] }\n",
    "                   {'y' = [position of the dataframe in the list] }\n",
    "    '''\n",
    "    dfs_return = []\n",
    "\n",
    "    # Run along the list of dataframes\n",
    "    for i, df in enumerate(dfs):\n",
    "        '''\n",
    "        # sort timestamp by ascending order\n",
    "        df.sort_values(by='timestamp', ascending=True, inplace=True)\n",
    "        # get min and max timestamp values given by the dataframe\n",
    "        min_ts, max_ts = df['timestamp'].min(), df['timestamp'].max()\n",
    "        # get date (str) equivalent for these min/max timestamps\n",
    "        min_date = get_df_related_value(df, 'timestamp', 'date', min_ts)\n",
    "        max_date = get_df_related_value(df, 'timestamp', 'date', max_ts)\n",
    "        # build a dataframe with min/max dates and index related to \n",
    "        #  the dataframe.\n",
    "        df_ts = pd.DataFrame(dict(\n",
    "                x = [min_date, max_date],\n",
    "                y = [i+1, i+1]\n",
    "        ))\n",
    "        '''\n",
    "        # get X/Y values\n",
    "        x_values = df['date'].tolist()\n",
    "        y_values = [i+1] * len(x_values)\n",
    "        # format X/Y for ploting\n",
    "        df_ts = pd.DataFrame(dict(\n",
    "                x = x_values,\n",
    "                y = y_values\n",
    "        ))\n",
    "        \n",
    "        # Store the dataframe into the the list of dataframes\n",
    "        dfs_return.append(df_ts)\n",
    "\n",
    "    return dfs_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - DATA VISUALIZATION - PLOTTING TIME SERIES\n",
    "\n",
    "# I build a function to format x/y values for plotting lines\n",
    "\n",
    "def plot_df_time_periods(dfs):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Plot a line chart showing the time period of every input \n",
    "         dataframe.\n",
    "    INPUT\n",
    "        dfs (list) is a list of dataframes such that every\n",
    "            dataframe is defined like:\n",
    "            {'x' = [values of ‘dates’ from the dataframe] ] }\n",
    "            {'y' = [position of the dataframe in the list] }\n",
    "    OUTPUT\n",
    "        nil (only display)\n",
    "    '''\n",
    "    # declare the figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Run along the list of dataframes containing plotting data\n",
    "    for i, df in enumerate(dfs):\n",
    "        # Add a line of result per dataframe\n",
    "        fig.add_trace(go.Scatter(x=np.array(df['x']),\n",
    "                                 y=np.array(df['y']),\n",
    "                                 mode='lines+markers',\n",
    "                                 name='file' + str(i+1)\n",
    "                                )\n",
    "                    )\n",
    "    # Define the context of the plot\n",
    "    titre = 'Difference of time period covered per data file'\n",
    "    fig.update_layout(title=titre,\n",
    "                    xaxis_title=\"time line\",\n",
    "                    yaxis_title=\"File#\",\n",
    "                    legend=dict(traceorder='reversed'))\n",
    "    fig.update_yaxes(range=[0.5, len(dfs)+0.5])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# SECTION 2 - DATA VISUALIZATION - PLOTTING TIME SERIES\n",
    "\n",
    "# plot time period for every input dataframe\n",
    "\n",
    "# Define the list of input dataframes\n",
    "df_list = [df_file_1, df_file_2, df_file_3, df_file_4, df_file_5]\n",
    "\n",
    "# Get data for plotting\n",
    "dfs = get_df_dates(df_list)\n",
    "\n",
    "# Plot the time periods\n",
    "plot_df_time_periods(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PRE-PROCESSING - Merging files\n",
    "\n",
    "\n",
    "# Build a function that provide the list of common values through\n",
    "#  several dataframes for a given column's label.\n",
    "\n",
    "'''Discussion\n",
    "It may take a long time so there is certainly means to re-factorize \n",
    " this part of the code.\n",
    "'''\n",
    "\n",
    "def get_common_values(dfs, label):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        get common values on the selectec label through all given \n",
    "         dataframes\n",
    "    INPUT\n",
    "        dfs (list) is the list of all dataframes.\n",
    "        label (string) is the name of the column for which I want\n",
    "         find common values.\n",
    "    OUTPUT\n",
    "        common_values is a numpy array of the common values\n",
    "    '''\n",
    "    common_values, all_values = [], []\n",
    "\n",
    "    # run along every dataframe to find all values through all\n",
    "    #  dataframes.\n",
    "    for df in dfs:\n",
    "        # get an array of values of the selected label\n",
    "        values = np.unique(np.array(df[label]))\n",
    "        # run along these values \n",
    "        for value in values:\n",
    "            # if the value is not in the list of all values, I add it \n",
    "            #  to this list\n",
    "            if value not in all_values:\n",
    "                all_values.append(value)\n",
    "    # all_values contains the list of all values of the column label \n",
    "    #  contained in all dataframes.\n",
    "\n",
    "    # Run along all values to find only common values through all\n",
    "    #  dataframes.\n",
    "    for value in all_values:\n",
    "        cnt = 0  # count the number of occurrence of this value\n",
    "        for df in dfs:\n",
    "            values = np.unique(np.array(df[label]))\n",
    "            if value in values:\n",
    "                cnt += 1\n",
    "        if cnt == len(dfs):\n",
    "            common_values.append(value)\n",
    "    \n",
    "    return np.array(common_values)\n",
    "\n",
    "\n",
    "\n",
    "# Test function get_common_values\n",
    "df1 = pd.DataFrame({'date': ['2020-05-12', '2020-06-13', '2021-07-06', '2021-08-01', '2022-01-22', '2022-02-23']})\n",
    "df2 = pd.DataFrame({'date': ['2020-05-13', '2020-06-13', '2021-07-06', '2021-08-01', '2022-01-22', '2022-02-23']})\n",
    "df3 = pd.DataFrame({'date': ['2020-05-13', '2020-06-13', '2021-07-06', '2021-08-01', '2022-01-22', '2022-02-24']})\n",
    "toto = get_common_values([df1, df2, df3], 'date')\n",
    "print('computed {}\\nexpected {}'.format(toto, np.array(['2020-06-13', '2021-07-06', '2021-08-01', '2022-01-22'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PRE-PROCESSING - Merging files\n",
    "'''\n",
    "# Here, i'm checking the range of the dates, to ensure that I will have\n",
    "# access to most recent data.\n",
    "common_dates = get_common_values([df_file_3, df_file_4], 'date')\n",
    "df_dates = pd.DataFrame({\"date\": common_dates})\n",
    "df_dates.sort_values(by=\"date\", ascending=True, inplace=True)\n",
    "\n",
    "# It finally appears that it ends at 27-Dec-2022 so I can use this\n",
    "#  combination.\n",
    "print(df_dates)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PRE-PROCESSING - Merging files\n",
    "\n",
    "# Merge the dataframes with 'date' and 'dep' as common columns.\n",
    "print(df_file_3.shape)\n",
    "print(df_file_4.shape)\n",
    "df = pd.merge(df_file_3, df_file_4, on=['date', 'dep'], how='inner')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA UNDERSTANDING - Post merging\n",
    "\n",
    "# get the list of columns of the resulting merged dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA UNDERSTANDING - Post merging\n",
    "\n",
    "# check the dataframe \n",
    "display_columns_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA UNDERSTANDING - Post merging\n",
    "\n",
    "# remove nan from the dataframe\n",
    "print(df.shape)\n",
    "df.dropna(axis='index', how='any', inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA UNDERSTANDING - Post merging\n",
    "\n",
    "# check the dataframe \n",
    "display_columns_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA UNDERSTANDING - Post merging\n",
    "\n",
    "This file contains a combination of main indicators for monitoring of\n",
    " the COVID-19 epidemic in France since 18 March 2020. Data are provided\n",
    " per department and per region of France. The file reports information\n",
    " related to COVID-19 testing campaign and patients at hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA UNDERSTANDING - Data Description - df_file_3:\n",
    "\n",
    "\n",
    "I present only remaining data at this stage.\n",
    "\n",
    "- Description of data - context:\n",
    "\n",
    "\t'date'    = date (object) when the information is given:\n",
    "\t\t\t    YYYY-MM-DD;\n",
    "\n",
    "\t'dep'     = number (str or int)of the french department;\t\n",
    "\n",
    "\t'reg'     = number (int) of the french region;\n",
    "\n",
    "\t'lib_dep' = name (object) of the french department;\n",
    "\n",
    "\t'lib_reg' = name (object) of the french region.\n",
    "\n",
    "\n",
    "- Description of data - Hospital situation:\n",
    "\n",
    "\t'hosp'         = number (int) of patients currently hospitalized\n",
    "\t\t\t\t\t  due to COVID-19;\n",
    "\n",
    "\t'incid_hosp'   = number (float) of new hospitalized patients in\n",
    "\t\t\t\t     the last 24h;\n",
    "\n",
    "\t'rea' is the   = number (int) of patients currently in\n",
    "\t\t\t\t\t resuscitation or intensive care unit;\n",
    "\n",
    "\t'incid_rea'    = number (float) of new patients who were admitted\n",
    "\t\t\t\t     to the resuscitation unit in the last 24h;\n",
    "\n",
    "\t'rad'          = cumulative number (int) of patients who where\n",
    "\t\t\t\t\t  hospitalized for COVID-19 but back to home due\n",
    "\t\t\t\t\t  to improvement of their health;\n",
    "\n",
    "\t'incid_rad'    = number (float) of the patients back to home in\n",
    "\t\t\t\t\t the last 24h;\n",
    "\n",
    "\t'reg_rea'       = undefined (int);\n",
    "\n",
    "\t'reg_incid_rea' = undefined (float).\n",
    "\n",
    "\n",
    "- Description of data - decease due to COVID-19:\n",
    "\n",
    "\t'dchosp'       = number (int) of decease at hospital;\n",
    "\n",
    "\t'incid_dchosp' = number (float) of new patients deceased at the\n",
    "\t\t\t\t\t hospital in the last 24h.\n",
    "\n",
    "\n",
    "- Description of data - tests:  \n",
    "  \n",
    "\t'pos'      = number (float) of people declared positive (D-3 date\n",
    "\t             of test);\n",
    "\n",
    "\t'pos_7j'   = number (float) of people declared positive on a week\n",
    "\t\t\t\t  (D-3 data of test);\n",
    "\n",
    "\n",
    "- Description of data - COVID-19 epidemic monitoring indicators:  \n",
    "  \n",
    "\t'tx_pos'   = Positivity rate (float) is the number of people tested\n",
    "\t\t\t      positive (RT-PCR or antigenic assay) for the first\n",
    "\t\t\t      time in the last 60 days over the number of people\n",
    "\t\t\t      tested (positive or negative) on a given period,\n",
    "\t\t\t      without being tested positive in the last 60 days;\n",
    "\t\t\t\t  \n",
    "\t'tx_incid' = Incidence rate (float) is the number of people tested\n",
    "\t\t\t      positive (RT-PCR or antigenic assay) for the first\n",
    "\t\t\t\t  time in the last 60 days over the size of population;\n",
    "\t\t\t\tit is given for 100 000 of inhabitants;\n",
    "\t\t\t\t  \n",
    "\t'TO' \t   = Occupancy rate (float) is the number of hostpitalized\n",
    "\t\t         COVID-19 patients over the initial number of beds at\n",
    "\t\t\t\t hospital (before increase of this number).\n",
    "\n",
    "\n",
    "Comment:\n",
    "'reg_rea' and 'reg_incid_rea' are not defined.\n",
    "I recommend removing them because I don't know their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA UNDERSTANDING - cleaning dataframe\n",
    "\n",
    "# As recommended, I remove useless data\n",
    "df.drop(columns=['reg_rea', 'reg_incid_rea'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA UNDERSTANDING - Data Description - df_file_4:\n",
    "\n",
    "\n",
    "'date' = date (object) when the information is given: YYYY-MM-DD;\n",
    "\t\n",
    "'dep' = number (str or int)of the french department;\n",
    "\n",
    "'n_dose1' = number of people who received their first dose of vaccine\n",
    "\n",
    "'n_complet' = number of people who received a primo-vaccination\n",
    "\n",
    "'n_rappel' = number of people primo-vaccinated who received at least\n",
    "             one booster dose prescribed for specific reason by a \n",
    "             health professional.\n",
    "\n",
    "'n_2_rappel' = number of people primo-vaccinated who received two\n",
    "               booster doses prescribed for specific reason by a health\n",
    "               professional.\n",
    "\n",
    "'n_3_rappel' = number of people primo-vaccinated who received three\n",
    "               booster doses prescribed for specific reason by a health\n",
    "               professional.\n",
    "\n",
    "'n_rappel_biv' = number of people vaccinated with a booster dose\n",
    "                 adjusted to omicron variants.\n",
    "\n",
    "'n_cum_dose1' = cumulative number of people who received one dose of\n",
    "                vaccin.\n",
    "\n",
    "'n_cum_complet' = cumulative number of people who received one dose of\n",
    "                  vaccin.\n",
    " \n",
    "'n_cum_rappel' = cumulative number of people primo-vaccinated who \n",
    "                 received at least one booster dose prescribed for\n",
    "                 specific reason by a health professional.\n",
    "\n",
    "'n_cum_2_rappel' = cumulative number of people primo-vaccinated who\n",
    "                   received at least one booster dose prescribed for\n",
    "                   specific reason by a health professional.\n",
    "\n",
    "'n_cum_3_rappel' = cumulative number of people primo-vaccinated who\n",
    "                   received three booster doses prescribed for specific\n",
    "                   reason by a health professional.\n",
    "\n",
    "'n_cum_rappel_biv' = cumulative number of people vaccinated with a\n",
    "                     booster dose adjusted to omicron variants.\n",
    "\n",
    "'couv_dose1' = vaccination coverage with first dose\n",
    "               = n_cum_dose1 / pop\n",
    "\n",
    "'couv_complet' = vaccination coverage with primo-vaccination\n",
    "                 = n_cum_complet / pop\n",
    "\n",
    "'couv_rappel' = vaccination coverage with first booster dose \n",
    "                = n_cum_rappel / pop\n",
    "\n",
    "'couv_2_rappel' = vaccination coverage with second booster dose \n",
    "                  = n_cum_2_rappel / pop\n",
    "\n",
    "'couv_3_rappel' = vaccination coverage with third booster dose \n",
    "                  = n_cum_3_rappel / pop\n",
    "\n",
    "'couv_rappel_biv' = vaccination coverage with a booster dose adjusted\n",
    "                    to omicron variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA UNDERSTANDING - Post merging\n",
    "\n",
    "# Create a dictionary with a shot description of every label for further\n",
    "#  use during visualization of results.\n",
    "df_dict_labels = {\n",
    "    'dep': 'Dept.',\n",
    "    'date': 'Date',\n",
    "    'reg': 'Region',\n",
    "    'tx_pos': 'Positivity rate',\n",
    "    'tx_incid': 'Incidence rate',\n",
    "    'TO': 'Hospital occupancy rate',\n",
    "    'hosp': 'Hospitalized People',\n",
    "    'rea': 'People in resuscitation/Intensice care',\n",
    "    'rad': 'Cumulative hospitalized people back to home',\n",
    "    'dchosp': 'Death at hospital',\n",
    "    'reg_rea': 'unknown',\n",
    "    'incid_hosp': 'Nes hospitalization in last 24h',\n",
    "    'incid_rea': 'New people in resuscitation/Intensice care in last 24h',\n",
    "    'incid_rad': 'New people hospitalized back to home in last 24h',\n",
    "    'incid_dchosp': 'New death in last 24h',\n",
    "    'reg_incid_rea': 'unknown',\n",
    "    'pos': 'Positive case (J-3)',\n",
    "    'pos_7j': 'Positive case on a week',\n",
    "    'n_dose1': 'Vaccinated with 1 dose',\n",
    "    'n_complet': 'Vaccinated with 2 doses',\n",
    "    'n_rappel': 'Vaccinated with 1 booster dose',\n",
    "    'n_2_rappel': 'Vaccinated with 2 booster doses',\n",
    "    'n_rappel_biv': 'Vaccinated with 1 booster dose (adj. Omicron)',\n",
    "    'n_3_rappel': 'Vaccinated with 3 booster doses',\n",
    "    'n_cum_dose1': 'Cumulative vaccinated with 1 dose',\n",
    "    'n_cum_complet': 'Cumulative vaccinated with 2 doses',\n",
    "    'n_cum_rappel': 'Cumulative vaccinated with 1 booster dose',\n",
    "    'n_cum_2_rappel': 'Cumulative vaccinated with 2 Booster doses',\n",
    "    'n_cum_rappel_biv': 'Cumulative vaccinated with 1 booster dose (adj. Omicron)',\n",
    "    'n_cum_3_rappel': 'Cumulative vaccinated with 3 Booster doses',\n",
    "    'couv_dose1': 'Vaccionation coverage with 1 dose',\n",
    "    'couv_complet': 'Vaccionation coverage with 2 doses',\n",
    "    'couv_rappel': 'Vaccionation coverage with 1 booster dose',\n",
    "    'couv_2_rappel': 'Vaccionation coverage with 2 booster doses',\n",
    "    'couv_rappel_biv':'Vaccionation coverage with 1 booster dose (adj. Omicron)',\n",
    "    'couv_3_rappel': 'Vaccionation coverage with 3 booster doses'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA CLEANING - Defining the needs\n",
    "\n",
    "As far as possible, I'd prefer working on dataset with the minimum\n",
    " missing values.\n",
    "\n",
    "So I will clean the data set before iniating processing of data.\n",
    "\n",
    "As I would like to build a prediction model, I'd like to use the\n",
    " cleanest data set possible, without invalid data (nan or infinite),\n",
    "  without new columns that would be only computation from existing data.\n",
    "In addition, as a priori, the prediction model will have to predict\n",
    " figures from figures, I will have to remove columns of string (unless\n",
    " I need to use through dummy values).\n",
    "\n",
    "Before adding new computed parameters to the dataframe, I copy the\n",
    " dataframe merged and cleaned for further use in modeling.\n",
    "\n",
    "\n",
    "First, it consists in building a function to clean the data set from\n",
    " unexpected values (nan, infinite) from the new computed data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA CLEANING - Detailing the actions\n",
    "\n",
    "Clean the data will first consists in removing useless data such as:\n",
    "\n",
    "Remove string values by their equivalent in numerical values when\n",
    " possible before adding dummy values (only if needed). As far as I can,\n",
    " I would prefer find a simple substitute rather than add simple but\n",
    " numerous data because it remains easier to handle for plotting and\n",
    " modeling.\n",
    "\n",
    "- Replacing string format of 'dep' by a pure numerical value (int);\n",
    "  two corner cases to consider: Corse with ids 2A and 2B; I propose\n",
    "  to replace them by 201 and 202.\n",
    "  ACTION: build a function that replace string values of dep by int\n",
    "          values.\n",
    "\n",
    "- 'lib_dep' and 'lib_reg': Indeed, I could use only the index number\n",
    "  and use a dictionary if needed to display the label of a deparment\n",
    "  of a region.\n",
    "  ACTION: Build dictionaries that give the label of department and\n",
    "          region according to their id number. Then build a function\n",
    "          that allows replacing an id or a list of ids by the labels.\n",
    "\n",
    "- Replace the date by timestamp values; I propose to compute this\n",
    "  timestamp as the number of second passed from year 0 to the date\n",
    "  (string); it would ease the use when considering analysis and\n",
    "  display with time series.\n",
    "  ACTION: Build a function that compute values of timestamp, adding\n",
    "          values in the dataframe.\n",
    "\n",
    "- Finally, I will study the opportunity to remove columns and rows\n",
    "  with missing values.\n",
    "  ACTION: Analyze content of the dataframe to detect invalid data\n",
    "          and decide if a can keep them or no.\n",
    "          After a first modeling, I could also check the weight of\n",
    "          every parameters to adjust/reduce content of the dataframe\n",
    "          for modeling purpose. It leads to think a dataframe that\n",
    "          would be specific for modeling.\n",
    "\n",
    "Once cleaned, I will copy the dataframe for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "id of french departments and region shall be convert into numerical\n",
    " values before building their dictionaries.\n",
    "\n",
    "Then labels of the departments and regions can be removed from the\n",
    " dataframe after having created a dictionary to find the label from\n",
    " the id.\n",
    "\n",
    "This action consist in following steps:\n",
    "\n",
    "- Replace/ensure that ids of department and region are numerical \n",
    "  values ((int) with no string.\n",
    "\n",
    "- Build two dictionaries (department and region) that give the label\n",
    "  from the id.\n",
    "\n",
    "- Build two functions (department and region) that allows questioning\n",
    "  these dictionaries to get the label from the id.\n",
    "\n",
    "- Build two functions (department and region) to generalize and\n",
    "  allowing getting a list of labels from a list of ids.\n",
    "  \n",
    "- Finally, once I have these dictionaries and functions, I can\n",
    "  remove the columns related to labels of departments and regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# I create a function that allows replacing str by int values\n",
    "\n",
    "def replace_str_id(df, label):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "\n",
    "    INPUT\n",
    "        df is the pandas dataframe\n",
    "        label (string) is the label of the column to process\n",
    "              'dep' or 'region'.\n",
    "    OUTPUT\n",
    "        df is the input pandas dataframe with string values of label\n",
    "        replace by int values.\n",
    "\n",
    "    comment:\n",
    "        think about specific Corse departments 2A and 2B\n",
    "        that I propose to replace by 201 and 202.\n",
    "\n",
    "    '''\n",
    "    dico = {'2A': 201, '2B': 202}\n",
    "\n",
    "    # get unique values of the label\n",
    "    values = np.array(df[label].values)\n",
    "\n",
    "    convert = []\n",
    "    # Run along the values for conversion from str to int\n",
    "    for value in values:\n",
    "        # It try to refers to the dictionary for Corse's deps\n",
    "        try:\n",
    "            # basically, I try to convert str to int\n",
    "            c = int(value)\n",
    "        except:\n",
    "            # when it fails,\n",
    "            if value in dico:\n",
    "                c = dico[value]\n",
    "            else:\n",
    "                # otherwise I apply value zero\n",
    "                c = 0\n",
    "        '''\n",
    "        if (value in dico_keys):\n",
    "            c = dico[value]\n",
    "        else:\n",
    "        '''\n",
    "        convert.append(c)\n",
    "\n",
    "    df[label] = np.array(convert)\n",
    "    df[label] = df[label].astype(int)  # Ensure format for the columns.\n",
    "                                       # Do not put before otherwise,\n",
    "                                       #  it fails to convert 2A and \n",
    "                                       #  2B and display and warning\n",
    "\n",
    "    return df\n",
    "\n",
    "# check the function\n",
    "df_test = pd.DataFrame({'dep': np.array(['01', 21, '2A', '2B', '196'])})\n",
    "result = replace_str_id(df_test, 'dep')\n",
    "solution = [1, 21, 201, 202, 196]\n",
    "print('computed: {}\\nexpected: {}'.format(np.array(result['dep']),\n",
    "                                          solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# I convert 'dep' and 'reg' to int values\n",
    "df = replace_str_id(df, 'dep')\n",
    "df = replace_str_id(df, 'reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# I would like to present results per department or per region.  I will\n",
    "#  build a dictionary to be able to link id of a department to the name\n",
    "#  of a department. I build a function to be able to get a dictionary\n",
    "#  either of department or for region. Then, I will add a function\n",
    "#  specially for department.\n",
    "\n",
    "def get_area_names(df, aera_id_label:str, aera_name_label: str):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        build a dictionary of a type or area (department or region)\n",
    "         allowing getting a name of this area from its id.\n",
    "    INPUT\n",
    "        df is the globale dataframe\n",
    "        aera_id_label is the id (str) of the area: 'dep' (department)\n",
    "                      or 'reg' (region);\n",
    "        area_name_label is the name (str) of the area: 'lib_dep'\n",
    "                        (department) or 'lib_reg' (region).\n",
    "    OUTPUT\n",
    "        dict_areas is a dictionary: id is the key, name is the value.\n",
    "    '''\n",
    "\n",
    "    dict_areas = dict()\n",
    "    # get the list of area id and related names\n",
    "    area_ids = np.array(df[aera_id_label])\n",
    "    area_names = np.array(df[aera_name_label])\n",
    "    area_id_uniq = []\n",
    "    for i, area_id in enumerate(area_ids):\n",
    "        if area_id not in area_id_uniq:\n",
    "            area_id_uniq.append(area_id)\n",
    "            dict_areas[area_id] = area_names[i]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return dict_areas\n",
    "\n",
    "# Check the function\n",
    "# - Get a dictionary of departments\n",
    "dict_lib_dep = get_area_names(df, 'dep', 'lib_dep')\n",
    "print('Department dictionary:\\n', dict_lib_dep)\n",
    "# - Get a dictionary of regions\n",
    "dict_lib_reg = get_area_names(df, 'reg', 'lib_reg')\n",
    "print('Region dictionary:\\n', dict_lib_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# Then I build a function dedicated to the identification of a name of\n",
    "#  the department according to its id. \n",
    "\n",
    "def get_dep_name(dep_id):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        give the name of a department in accordance with its id\n",
    "    INPUT\n",
    "        dep_id is the id (int) of the department from\n",
    "               which we want the name.\n",
    "    OUTPUT\n",
    "        dep_name is the name (str) of the department related to the\n",
    "                 given id.\n",
    "    '''\n",
    "    try:\n",
    "        name = dict_lib_dep[dep_id]\n",
    "    except:\n",
    "        name = 'unknown'\n",
    "    return name\n",
    "\n",
    "# Check the function with various formats\n",
    "\n",
    "t_int = get_dep_name(45)\n",
    "t_cor = get_dep_name(201)\n",
    "t_unk = get_dep_name(600)\n",
    "print('Loiret: {} - Corse-du-Sud: {} - unknown: {}'.format(t_int, t_cor, t_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# Then I build a function dedicated to the identification of a name of\n",
    "#  the region according to its id. \n",
    "\n",
    "def get_reg_name(reg_id):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        give the name of a region in accordance with its id\n",
    "    INPUT\n",
    "        reg_id is the id (str, int or float) of the region from which\n",
    "               we want the name.\n",
    "    OUTPUT\n",
    "        reg_name is the name (str) of the region related to the given\n",
    "                 id.\n",
    "    '''\n",
    "    try:\n",
    "        name = dict_lib_reg[reg_id]\n",
    "    except:\n",
    "        name = 'unknown'\n",
    "    return name\n",
    "\n",
    "# Check the function with various formats\n",
    "t_int = get_reg_name(93)\n",
    "t_unk = get_reg_name(604)\n",
    "solution = \"Provence-Alpes-Côte d'Azur\"\n",
    "print(\"{}: {} - unknown: {}\".format(solution, t_int, t_unk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# Then by extension, I build a function to provide the name of\n",
    "#  departments from a list of their ids.\n",
    "\n",
    "def get_dep_names(dep_ids: list):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        give a list of department names according to a list of\n",
    "         department ids.\n",
    "    INPUT\n",
    "        dep_ids is the list of department id\n",
    "    OUTPUT\n",
    "        dep_names is the list of department names\n",
    "    '''\n",
    "    dep_names = list()\n",
    "    for dep_id in dep_ids:\n",
    "        dep_names.append(get_dep_name(dep_id))\n",
    "\n",
    "    return dep_names\n",
    "\n",
    "# Check the function\n",
    "list_dep_ids = [45, 21]\n",
    "solution = ['Loiret', \"Côte-d'Or\"]\n",
    "print('expected: {} vs computed: {}'.format(solution,\n",
    "                                        get_dep_names(list_dep_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# In the same way, I build a function to provide the name of region from\n",
    "#  a list of their ids.\n",
    "\n",
    "def get_reg_names(reg_ids: list):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        give a list of region names according to a list of region ids\n",
    "    INPUT\n",
    "        reg_ids is the list of region id\n",
    "    OUTPUT\n",
    "        reg_names is the list of region names\n",
    "    '''\n",
    "    reg_names = list()\n",
    "    for reg_id in reg_ids:\n",
    "        reg_names.append(get_reg_name(reg_id))\n",
    "\n",
    "    return reg_names\n",
    "\n",
    "#  Check the function\n",
    "list_reg_ids = [76, 27]\n",
    "solution = ['Occitanie', 'Bourgogne et Franche-Comté']\n",
    "print('expected: {} vs computed: {}'.format(solution,\n",
    "                                        get_reg_names(list_reg_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Building Department & Region dictionaries\n",
    "\n",
    "# Finally I can remove labels of department and region since I have\n",
    "#  their dictionaries.\n",
    "size_before = df.shape\n",
    "df.drop(columns=['lib_dep', 'lib_reg'], inplace=True)\n",
    "\n",
    "# check result of the action:\n",
    "title = \"df's size has changed from\"\n",
    "print(\"{} {} to {}:\".format(title, size_before, df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA CLEANING - Adding timestamp\n",
    "\n",
    "For visualization, I need to convert string dates into an int value of\n",
    " number of days (timestamp). For that purpose I will split the string\n",
    " into the year, the month and the day and compute the number of passed\n",
    " days for every of these part since their 0-ref.\n",
    "\n",
    "For this purpose, I need:\n",
    "\n",
    "- A function that split the string dates and convert this into the\n",
    "  equivalent number of days passed since days 0 of year 0; for that\n",
    "  purpose, I need to convert year and month in number of passed days.\n",
    "  \n",
    "- A function that gives the number of passed days to beginning of the\n",
    "  selected month from the beginning of the year; I can use a kind of\n",
    "  calendar that gives the number of passed from beginning of the day\n",
    "  for every month.\n",
    "\n",
    "For modeling, I could use the timestamp but do not use the 'date'\n",
    " (str). I shall remove the column 'date' from the dataframe dedicated\n",
    " for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Adding timestamp\n",
    "\n",
    "# Create a kind of calendar that provide the number of cumumlated passed\n",
    "#  days at beginning of every month since beginning of the year.\n",
    "\n",
    "def get_cumul_days():\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Return a list to ease count, month per month, passed day from\n",
    "         the beginning of the year.\n",
    "    INPUT\n",
    "        nil\n",
    "    OUTPUT\n",
    "        cnt_day_at_start_month is a list that gives the nb of past days\n",
    "         from beginning of the year for every month.\n",
    "    '''\n",
    "    cnt_day_at_start_month = []\n",
    "\n",
    "    day_by_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in day_by_month:\n",
    "        cnt += i \n",
    "        cnt_day_at_start_month.append(cnt)\n",
    "\n",
    "    return cnt_day_at_start_month\n",
    "\n",
    "# Chech the function\n",
    "calendar = get_cumul_days()\n",
    "print('calender:', calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Adding timestamp\n",
    "\n",
    "# Then I use this calendar to build a function that will provide the\n",
    "#  number of passed days since beginning of the year to beginning of\n",
    "#  the selected month.\n",
    "\n",
    "def get_cnt_day_at_start_month(m, calendar):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        get the number of past days from the beginning of year and at \n",
    "         beginning of the selected month.\n",
    "    INPUT\n",
    "        m is the selected month (str of int, int or float)\n",
    "        calendar is a list that gives the nb of past days from\n",
    "                 beginning of the year for every month\n",
    "    OUTPUT\n",
    "        cnt is the count of days (int) from beginning of the year at\n",
    "         start of the selected month.\n",
    "    '''\n",
    "    cnt = calendar[int(m)-1]\n",
    "    return cnt\n",
    "\n",
    "# Check the function\n",
    "# 31 days passed at beginning of february.\n",
    "result = get_cnt_day_at_start_month(2, calendar)\n",
    "print('result: {} vs expected: {}'.format(result, 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Adding timestamp\n",
    "\n",
    "# Finally, I compute a timestamp from the dates as the number of days\n",
    "#  passed  from 0 to the selected day such as for instances:\n",
    "# 2022-06-24 => 2022 * nb_days_passed_to_this_year from year 0\n",
    "#               + nb_passed_days_from_begin_of_the_year_to_begin_of\n",
    "#                 this_month\n",
    "#               + 24 (nb of days passed since beginning of the month)\n",
    "#               = 2022 * 365.24219 + 151 + 24 = 738695\n",
    "# 0000-01-01 => 0 + 0 + 1 = 1\n",
    "\n",
    "\n",
    "def get_timestamp_array(dates_str):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Convert an np.array of dates in string format\n",
    "        to a timestamp in number of days.\n",
    "    INPUT\n",
    "        dates_str is a np.array of dates YYYY-MM-DD in string format\n",
    "    OUTPUT\n",
    "        timestamp is np.array with data converted in number of days (int)\n",
    "    '''\n",
    "    timestamp = []\n",
    "    j = 0\n",
    "    for d in dates_str:\n",
    "        try:\n",
    "            y = float(d[:4])\n",
    "            m = float(d[5:7])\n",
    "            d = float(d[8:10])\n",
    "            # convert the data in number of days\n",
    "            cnt_days_for_months = float(get_cnt_day_at_start_month(m, calendar))\n",
    "            ts = y * 365.24219 + cnt_days_for_months + d\n",
    "        except:\n",
    "            ts = 0.0\n",
    "        ts_r = round(ts, 0)\n",
    "        timestamp.append(int(ts_r))\n",
    "        j += 1\n",
    "    return np.array(timestamp)\n",
    "\n",
    "# Check the function\n",
    "test_dates = np.array(['2022-06-24', '0000-01-01'])\n",
    "timestamp_arr = get_timestamp_array(test_dates)\n",
    "print('expected: {} vs computed {}'.format([738695, 1], timestamp_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - Adding timestamp\n",
    "\n",
    "# Add a timestamp column in df\n",
    "# It will hep for displaying data in time series and find most recent\n",
    "#  values per department.\n",
    "df['timestamp'] = get_timestamp_array(np.array(df['date']))\n",
    "# update the labels dictionary\n",
    "df_dict_labels['timestamp'] = 'Timestamp'\n",
    "# As a reminder, I've decided to keep the column 'date' for avoid \n",
    "#  need to rebuild the date value in string format from the timestamp\n",
    "#  when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA CLEANING - analysis of residual missing values\n",
    "\n",
    "Analyze content of the dataframe to detect invalid data and decide if\n",
    " a can keep them or no.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - analysis of residual missing values\n",
    "\n",
    "display_columns_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA CLEANING - analysis of residual missing values\n",
    "\n",
    "The function raises that following labels have 0.27% missing values:\n",
    "tx_pos, tx_incid, pos, pos_7j\n",
    "\n",
    "I assume we can remove rows with missing values in these columns without\n",
    " a significant impact on forcoming results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA CLEANING - remove residual missing values\n",
    "\n",
    "size_before = df.shape\n",
    "# I remove all rows containing at least one nan\n",
    "df.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "# check result of the action:\n",
    "title = \"df's size has changed from\"\n",
    "print(\"{} {} to {}:\".format(title, size_before, df.shape))\n",
    "\n",
    "# Check result of the action\n",
    "display_columns_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# DATA ANALYSIS - time\n",
    "\n",
    "I would compare the time period covered by every data file to illustrate\n",
    " their differences.\n",
    "\n",
    "I need to add timestamp column in every data file ,\n",
    " then order the dataframe on this timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA ANALYSIS - time\n",
    "\n",
    "# Add timestamp to every dataframe\n",
    "# df_file_3, df_file_4, df_file_2, df_file_1\n",
    "\n",
    "df_file_3['timestamp'] = get_timestamp_array(\n",
    "                              np.array(df_file_3['date']))\n",
    "df_file_4['timestamp'] = get_timestamp_array(\n",
    "                              np.array(df_file_4['date']))\n",
    "df_file_2['timestamp'] = get_timestamp_array(\n",
    "                              np.array(df_file_2['date']))\n",
    "df_file_1['timestamp'] = get_timestamp_array(\n",
    "                              np.array(df_file_1['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA ANALYSIS - time\n",
    "\n",
    "# Build a function to link timestamp to date\n",
    "\n",
    "def get_df_related_value(df, label_in, label_out, value_in_ref):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        return value of a column from a dataframe at the same\n",
    "         index that I find a given value in another column of \n",
    "         this dataframe.\n",
    "    INPUT\n",
    "        df is a pandas dataframe;\n",
    "        label_in (string) is the label of the column where to search\n",
    "                 the value value_in_ref;\n",
    "        label_out (string) is the label of the column from where\n",
    "                  I return the value at the same index where I find\n",
    "                  value_in_ref;\n",
    "        value_in_ref (str, int or float) is the value to search for in\n",
    "                     column named label_in.\n",
    "    OUTPUT\n",
    "        value_out (str, int or float) is the value from column\n",
    "                  label_out at the index where I find the value_in_ref.\n",
    "\n",
    "    comment:\n",
    "        I take the first position found. \n",
    "    '''\n",
    "    value_out = np.nan\n",
    "\n",
    "    for i, value in enumerate (np.array(df[label_in])):\n",
    "        if value == value_in_ref:\n",
    "            value_out = np.array(df[label_out].values)[i]\n",
    "            break\n",
    "    return value_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA PROCESSING - prepare dataframe for processing and analysis\n",
    "\n",
    "\n",
    "Before starting data processing, I shall:\n",
    "- I group the dataframe by department (if no dummy method applied to \n",
    "  the departments) and by time (timestamp).\n",
    "- Create a list of unique departments for preparing the analysis per\n",
    "  department.\n",
    "- I've indicated that the dataframe shall be copied to own a dataframe\n",
    "  dedicated for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - Prepare dataframe\n",
    "# \n",
    "# Group the data set by dep and by timestamp to ease searching results by\n",
    "#  department and by time.\n",
    "\n",
    "if 'dep' in df.columns:\n",
    "    df.groupby(['dep', 'timestamp'])  # If no dummy applied\n",
    "else:\n",
    "    df.groupby(['timestamp'])  # If dummy applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - Prepare plotting\n",
    "\n",
    "# Get the unique list of departments to get results for every department\n",
    "deps = pd.unique(df['dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - Prepare modeling\n",
    "\n",
    "# As mentioned before, last action of the cleaning phase consists in \n",
    "#  copy the dataframe to have a specific dataframe for modeling while \n",
    "#  the other dataframe will be used for calculation and addition of\n",
    "#  data.\n",
    "\n",
    "# Copy df for modeling\n",
    "df_mdl = df.copy(deep=True)\n",
    "\n",
    "# As mentioned before, remove the column 'date' (str):\n",
    "df_mdl.drop(columns='date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA PROCESSING - Define the needs\n",
    "\n",
    "According to this description, it may be possible to compute the number\n",
    " of people in the population as\n",
    " n_cum_dose1 / couv_dose1 = pop  (new)\n",
    "I choose this mean because n_cum_dose1 must be higher than other\n",
    " n_cum_xxx such I get the highest accuracy possible with this data set.\n",
    "\n",
    "We can calculate:\n",
    "-\tthe population, label ‘pop’:\n",
    "  pop = n_cum_dose1 / couv_dose1\n",
    "I choose this mean because n_cum_dose1 must be higher than other n_cum_xxx such I get the highest accuracy possible with this data set.\n",
    "I will check it did not vary too much along the time\n",
    "\n",
    "-\tthe rate of the population which is fully vaccinated (two doses), label ‘tx_f_vacc’:\n",
    "  tx_f_vacc = n_cum_complet / pop\n",
    "\n",
    "-\tthe rate of the population which is hospitalized, label ‘tx_hosp’;\n",
    "  tx_hosp  = hosp / pop\n",
    "\n",
    "-\tthe rate of population positive to COVID-19, label ‘tx_pos’:\n",
    "  tx_pos = pos / pop\n",
    "\n",
    "-\tthe rate of people in resuscitation or intensive care, label ‘tx_rea’:\n",
    "  tx_rea = rea / pop\n",
    "\n",
    "-\tthe rate of mortality at hospital, label ‘tx_dchosp’:\n",
    "  tx_dchosp = dchosp / pop\n",
    "\n",
    "\n",
    "Since I'm working with a cleaned dataframe, I'd try keeping this\n",
    " dataframe as clean as possible.\n",
    "I will ensure that adding computed values does not bring invalid data\n",
    " (nan or infinite). I can ensure that by adding systematically a\n",
    " function to clean the result of a computation.\n",
    " ACTION: Build a function to clean result of computation.\n",
    "\n",
    " I also need to prepare data for visualization\n",
    " ...\n",
    " ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - ensure computed data validity\n",
    "\n",
    "# This function allows removing unexpected values from new computed\n",
    "#  columns.\n",
    "\n",
    "def replace_invalid_data(df, label):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Replace invalid data like nan or infinite by values.\n",
    "    INPUT\n",
    "        df is the dataframe to work on;\n",
    "        label of the category (string) where invalid data will be\n",
    "         replaced.\n",
    "    OUTPUT\n",
    "        df is the dataframe modified.\n",
    "    '''\n",
    "    # Replace nan values by 0\n",
    "    df[label] = df[label].fillna(0)\n",
    "\n",
    "    # get the maximum ot he values which is non-infinite\n",
    "    #  to ensure providing a finite value as maximum when needed\n",
    "    serie = list()\n",
    "    for val in df[label]:\n",
    "        if val != np.inf:\n",
    "            serie.append(val)\n",
    "    maxi = max(serie)\n",
    "    mini = min(serie)\n",
    "    # Replace infinite (inf) values by the min/max of the serie\n",
    "    df[label].replace({np.inf: maxi, -np.inf: mini}, inplace=True)  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Since a use several times the operation of division with other \n",
    "# functions. I can build a specific function\n",
    "\n",
    "def divide_df(df, label_num, label_denum, label_result,\n",
    "              description=False):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Divide two columns of a dataframe according to their label \n",
    "        and push the result into the df undder a new label\n",
    "    INPUT\n",
    "        df is the pandas dataframe;\n",
    "        label_num (string) is the column's label used for numerator \n",
    "                  of the division;\n",
    "        label_denum (string) is the column's lael used for denumerator \n",
    "                    of the division;\n",
    "        label_result (string) is the label of the new column created\n",
    "                     for implementing result of the division;\n",
    "        description (bool) is the status to display (True) or not\n",
    "                    (False) the statistical description of the new \n",
    "                    column: Default = False.\n",
    "    OUTPUT\n",
    "        df is the input pandas dataframe completed with the new colum\n",
    "           providing result of the devision.\n",
    "    comment:\n",
    "        div function allows supporting missing value (in case I miss\n",
    "         something) with fill_value.\n",
    "    '''\n",
    "\n",
    "    # do the computation into the new column\n",
    "    df[label_result] = df[label_num].div(df[label_denum])\n",
    " \n",
    "    # Replace invalid data\n",
    "    df = replace_invalid_data(df, label_result)\n",
    "\n",
    "    # Display statistics related to the new column\n",
    "    if description:        \n",
    "        print(\"Statistics related to '{}':\\n{}\".format(label_result,\n",
    "                                        df[label_result].describe()))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the number of people in the population according to\n",
    "#  pop = n_cum_dose1 / couv_dose1\n",
    "df = divide_df(df, 'n_cum_dose1', 'couv_dose1', 'pop', description=True)\n",
    "# Multiply by 100 to compensate couv_dose1 given in %\n",
    "df['pop'] = np.multiply(100.0, np.array(df['pop']))\n",
    "# update the labels dictionary\n",
    "df_dict_labels['pop'] = 'Population'\n",
    "\n",
    "# Description of the file in the web site indicate that \n",
    "#  the reference Population considered is the one given by INSEE\n",
    "#  at 1st January 2020, at 1st January 2021 and at 1st January 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_last = np.array(df[df['dep']== 31]['pop'])[-1]\n",
    "print(population_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA PROCESSING - Discussion\n",
    "\n",
    "It appears that some values of 'pop' give zero due to 0-value for at least one\n",
    " of both values used for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of people fully vaccinated\n",
    "#  tx_f_vacc = n_cum_complet / pop\n",
    "df = divide_df(df, 'n_cum_complet', 'pop', 'tx_f_vacc', description=True)\n",
    "# Convert the result in %\n",
    "df['tx_f_vacc'] = np.multiply(100.0, np.array(df['tx_f_vacc']))\n",
    "# update the labels dictionary\n",
    "df_dict_labels['tx_f_vacc'] = 'Rate of population fully vaccinated (%)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of hospitalization\n",
    "#  tx_hosp = hosp / pop\n",
    "df = divide_df(df, 'hosp', 'pop', 'tx_hosp', description=True)\n",
    "# Convert the result in %\n",
    "df['tx_hosp'] = np.multiply(100.0, np.array(df['tx_hosp']))\n",
    "# update the labels dictionary\n",
    "df_dict_labels['tx_hosp'] = 'Rate of hospitalization (%)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of COVID-19 positive \n",
    "#  tx_hosp = pos / pop\n",
    "df = divide_df(df, 'pos', 'pop', 'tx_pos', description=True)\n",
    "# Convert the result in %\n",
    "df['tx_pos'] = np.multiply(100.0, np.array(df['tx_pos']))\n",
    "# update the labels dictionary\n",
    "df_dict_labels['tx_pos'] = 'Rate of COVID-19 positive (%)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of hospitalization\n",
    "#  tx_hosp = hosp / pop\n",
    "df = divide_df(df, 'tx_hosp', 'tx_pos', 'rate_conv_hosp', description=True)\n",
    "# update the labels dictionary\n",
    "df_dict_labels['rate_conv_hosp'] = 'Rate of conversion from positive to hospitalization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of people in resuscitation or intensive care \n",
    "#  tx_rea = rea / pop\n",
    "df = divide_df(df, 'rea', 'pop', 'tx_rea', description=True)\n",
    "# Convert the result in %\n",
    "df['tx_rea'] = np.multiply(100.0, np.array(df['tx_rea']))\n",
    "# update the labels dictionary\n",
    "title = 'Rate of people in resuscitation or intensive care (%)'\n",
    "df_dict_labels['tx_rea'] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of people in resuscitation or intensive care \n",
    "#  tx_dchosp = dchosp / pop\n",
    "df = divide_df(df, 'dchosp', 'pop', 'tx_dchosp', description=True)\n",
    "# Convert the result in %\n",
    "df['tx_dchosp'] = np.multiply(100.0, np.array(df['tx_dchosp']))\n",
    "# update the labels dictionary\n",
    "df_dict_labels['tx_dchosp'] = 'Rate of death at hospital (%)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "DATA PROCESSING - Preparing visualization\n",
    "\n",
    "A part of the data processing consists in preparating and formating \n",
    " data for visualization.\n",
    "\n",
    "I shall prepare two types of visualizations:\n",
    "- time series\n",
    "- maps (result per departments) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='pop', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='tx_f_vacc', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='tx_hosp', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='tx_pos', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='rate_conv_hosp', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='tx_rea', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - TIME SERIES\n",
    "\n",
    "# Plot time series\n",
    "d_list = [9, 12, 31, 32, 46, 65, 81, 82]\n",
    "plot_time_series(df, label='tx_dchosp', departments=d_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - Preparing visualization for maps\n",
    "\n",
    "# I want to display computed values at the most recent date\n",
    "#  so for that purpose, I need to identify the last date and corresponding \n",
    "#  values for the selected category (column).\n",
    "\n",
    "def get_values_at_last_date(df, category, deps):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Provide values of the selected category at the last date available in\n",
    "         the dataframe, for every department.\n",
    "    INPUT\n",
    "        category is the name of the columns for which we want values at the \n",
    "         last date for every department\n",
    "    OUPUT\n",
    "        values is a dictionary with {dep: {'date': date, 'value': value of the\n",
    "         category at the stored date}}\n",
    "    '''\n",
    "\n",
    "    values = dict()\n",
    "\n",
    "    # format a dedicated list of data for the search\n",
    "    lst = [np.array(df['dep']),\n",
    "        np.array(df['date']),\n",
    "        np.array(df['timestamp']),\n",
    "        np.array(df[category])]\n",
    "\n",
    "    # Run over every department\n",
    "    for dep in deps:\n",
    "        time_ts = 0\n",
    "        id_max_date = 0\n",
    "        # run along the rows of the dataframe\n",
    "        for i, j in enumerate(lst[0]):\n",
    "            # When I met the selected department\n",
    "            #  and the timestamp is higher (ensure get finally the most\n",
    "            #  recent timestamp/date and related values of the category),\n",
    "            #  we stored date and value of the category in values, relatively\n",
    "            #  to the department. \n",
    "            if (j == dep) and (time_ts < lst[2][i]):\n",
    "                id_max_date = i\n",
    "                time_ts = lst[2][i]\n",
    "\n",
    "        values[dep] = {'date': lst[1][i], 'value': lst[3][i]}\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - Preparing visualization for maps\n",
    "\n",
    "# Convert result for visualization, I need to get values like\n",
    "#  date, {dep: values, ...}\n",
    "\n",
    "def format_values_for_vizu(dico_values, nb_decimal):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        format values for a vizualization on a map\n",
    "    INPUT\n",
    "        dico_values is a dictionary such {dep: {'date': date, 'value': value\n",
    "         of the category at the stored date}}\n",
    "    OUTPUT\n",
    "        series is a dictionary such as {date1: {depY: valueY, ...}, date2: {...}}\n",
    "    '''\n",
    "\n",
    "    series = dict()\n",
    "\n",
    "    dates = list()\n",
    "    for dep, j in dico_values.items():\n",
    "        date = j['date']\n",
    "        if date not in dates: # Create a sub-dictionary at every new date\n",
    "            dates.append(date)\n",
    "            series[date] = dict()\n",
    "        series[date][dep] = round(j['value'], nb_decimal)\n",
    "\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------     CHECK IS STILL USEFUL\n",
    "# DATA PROCESSING - Preparing visualization for time series\n",
    "\n",
    "# I need to get all values of a categories along the time for a selected department\n",
    "\n",
    "def get_timeserie(category, dep, duration_week):\n",
    "    \n",
    "    # get dataframe the selecte category with time and departments\n",
    "    df_ = df[['dep', 'date', 'timestamp', category]]\n",
    "\n",
    "    # keep only rows with the selected dep\n",
    "    df_ = df_[df_['dep'] == dep]\n",
    "\n",
    "    # sort dataframe by descending timestamp (most recent at first)\n",
    "    df_.sort_values(by=['timestamp'], ascending=False, inplace=True)\n",
    "    \n",
    "    # get last timestamp (most recent date)\n",
    "    timestamp_max = df_['timestamp'].values[0]\n",
    "\n",
    "    # compute start of the time window\n",
    "    timestamp_min = timestamp_max - 7*duration_week  # unit here is the day\n",
    "                                                     # 7 days per week\n",
    "\n",
    "    # keep only dataframe on the required time stamp period\n",
    "    df_ = df_[df_['timestamp'] > timestamp_min]\n",
    "\n",
    "    # get expected values and related dates\n",
    "    dates = df_['date'].values.tolist()\n",
    "    values = df_[category].values.tolist()\n",
    "    # revers is for getting time flow from left to right\n",
    "\n",
    "    return list(reversed(dates)), list(reversed(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA VISUALIZATION - PLOTTING ON MAP\n",
    "\n",
    "# Build a generic function for plotting figures on a map per department\n",
    "\n",
    "def plot_last_figures_on_map(df, label):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Provide the final data set to display figures on a map per\n",
    "         departement.\n",
    "    INPUT\n",
    "        df is the pandas dataframe\n",
    "        label (string) is the name column's name for which I want to\n",
    "              display figures.\n",
    "    OUTPUT\n",
    "        nil\n",
    "    '''\n",
    "    # format for vizualization\n",
    "    last_values_per_dep = get_values_at_last_date(df, label, deps)\n",
    "    # Get the selected data\n",
    "    values_visu = format_values_for_vizu(last_values_per_dep, 3)\n",
    "    # Data for visualization\n",
    "    print('{}:\\n{}'.format(label, values_visu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - DUMMY\n",
    "# \n",
    "# Try to get dummies on dep\n",
    "# Since there is a lot, wait and see.\n",
    "\n",
    "def dummy_data(df, cat):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        add dummy data to df removing the source of dummy\n",
    "    INPUT\n",
    "        df is the initial dataframe to work on\n",
    "        cat is the name of the category we want dummying\n",
    "    OUTPUT\n",
    "        df_dummy is the dataframe as copy of df but with dummy columns,\n",
    "         removing the source of dummy\n",
    "    '''\n",
    "    df_dummy = df.copy(deep=True)\n",
    "\n",
    "    # Create a new dataframe for preparing dummies from a copy of the \n",
    "    #  selected category \n",
    "    df_cat = pd.DataFrame([])\n",
    "    df_cat[cat] = np.array(df[cat])\n",
    "    print('shape df_cat:', df_cat.shape)\n",
    "\n",
    "    # Convert category values to just numbers 0 or 1 (dummy values)\n",
    "    #  of category\n",
    "    df_dum = pd.get_dummies(df_cat[cat], dtype=int)\n",
    "    print('shape df_dum:', df_dum.shape)\n",
    "\n",
    "    for col in df_dum.columns:\n",
    "        df_dummy[col] = np.array(df_dum[col])\n",
    "    print('shape df_dummy:', df_dummy.shape)\n",
    "\n",
    "    # Drop the original category column from `df_dummy`\n",
    "    df_dummy.drop(columns=cat, inplace=True)\n",
    "    print('shape df_dummy drop:', df_dummy.shape)\n",
    "\n",
    "    # check number of duplicates\n",
    "    #  before trying removing duplicates\n",
    "    duplicates = df_dummy.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df_dummy.drop_duplicates(keep='first', inplace=True)\n",
    "    print('shape df_dummy dupl:', df_dummy.shape)\n",
    "\n",
    "    # clean df_dummy from rows with full nan values\n",
    "    #  otherwise, at least the row is a nana row.\n",
    "    df_dummy.dropna(axis='index', how='all', inplace=True)\n",
    "    print('shape df_dummy dropna:', df_dummy.shape)\n",
    "\n",
    "    return df_dummy\n",
    "\n",
    "\n",
    "# Add dummy values of 'dep'\n",
    "# df_mdl = dummy_data(df_mdl, 'dep')\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - CLEANING\n",
    "\n",
    "# Clean the dataframe dedicated to model \n",
    "print(df_mdl.shape)\n",
    "\n",
    "# Remove the date (str) and dep (str) not supported for\n",
    "#  the linear regression model.\n",
    "cols = ['date', 'dep', 'reg', 'dchosp', 'rad', 'incid_rad']\n",
    "for col in cols:\n",
    "    try:\n",
    "        df_mdl.drop(columns=col, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Remove unexpected values from the dataframe dedicated to model\n",
    "df_mdl.dropna(axis='index', how='any', inplace=True)\n",
    "\n",
    "# Check result of the action\n",
    "print(df_mdl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use timestamp for removing the beginning og the period.\n",
    "# The analysis of data shows starting data are not reliable.\n",
    "\n",
    "# I fix the limit to 15-January-2021, i.e. timestamp 738170.\n",
    "\n",
    "df_mdl = df_mdl[df_mdl['timestamp'] > 738170]\n",
    "print(df_mdl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Build a model\n",
    "\n",
    "# Split the dataset into explanatory and response data sets\n",
    "\n",
    "def get_X_y(df: object, response:str):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Split df into exploratory X data and response y data\n",
    "    INPUT\n",
    "           df : pandas dataframe\n",
    "           response : target variable\n",
    "    OUTPUT\n",
    "           X : A matrix holding all of the variables you want to\n",
    "               consider when predicting the response\n",
    "           y : the corresponding response vector\n",
    "    '''\n",
    "    # output\n",
    "    X, y = None, None\n",
    "\n",
    "    # Get the Response var\n",
    "    if response in df.columns:\n",
    "        # Split into explanatory and response variables (1/2)\n",
    "        #  Get response variable\n",
    "        y = df[response]\n",
    "        df = df.drop(columns=[response])  # Remove pred_name from df\n",
    "    else:\n",
    "        print(\"\\tCAUTION! Unable to find the response in df\")\n",
    "        y = None\n",
    "\n",
    "    # Get the Exploratory vars\n",
    "    # Split into explanatory and response variables (2/2)\n",
    "    #  Get the input variables i.e. at this level just a copy of df\n",
    "    X = df.copy(deep=True)\n",
    "\n",
    "\n",
    "    if X is None:\n",
    "        print('\\tCAUTION! No X found')\n",
    "    if y is None:\n",
    "        print(' - No y found')\n",
    "\n",
    "    del df, response\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Get a trained model\n",
    "\n",
    "def get_model(X: object, y: object, testrate=.3):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Returns a linear prediction model according to train data\n",
    "    INPUT\n",
    "           X          : explanatory variables object\n",
    "           y          : response variable object\n",
    "           testrate   : proportion of the dataset to include in\n",
    "                         the test split,between 0.0 and 1.0;\n",
    "                         default value = 0.3\n",
    "    OUTPUT\n",
    "            model : linear regression model object from sklearn\n",
    "            list of X_train and y_train\n",
    "            list of X_test and y_test\n",
    "    '''\n",
    "    model = None\n",
    "    Xtrain, Xtest, ytrain, ytest = None, None, None, None\n",
    "\n",
    "    # sub-variables\n",
    "    y_pred, split = None, False\n",
    "    \n",
    "    if (X is not None) and (y is not None):\n",
    "        # Split into train and test X/y data set\n",
    "        #  to establish the model and score it\n",
    "        print(\"- Training\")\n",
    "        Xtrain, Xtest, ytrain, ytest = train_test_split(X, y,\n",
    "                                                        test_size=testrate,\n",
    "                                                        random_state=42)\n",
    "        if (Xtrain is not None) and (Xtest is not None) and \\\n",
    "           (ytrain is not None) and (ytest is not None):\n",
    "            split = True\n",
    "\n",
    "        # Establish model\n",
    "        print(\"- Modeling\")\n",
    "        if split:\n",
    "\n",
    "            # Linear model from scikit-learn:\n",
    "            #  https://scikit-learn.org/stable/modules/linear_model.html#\n",
    "\n",
    "            # Linear Regression\n",
    "            model = LinearRegression()\n",
    "\n",
    "            # Linear model Lasso\n",
    "            # model = linear_model.Lasso(alpha=0.1)\n",
    "\n",
    "            # Ridge regression and classification\n",
    "            # model = linear_model.Ridge(alpha=.5)\n",
    "\n",
    "            # BayesianRidge()\n",
    "            # model = linear_model.BayesianRidge()\n",
    "\n",
    "            # LogisticRegression\n",
    "            # model = linear_model.LogisticRegression(random_state=0)\n",
    "            # very long -> not achieve\n",
    "\n",
    "            # generalized linear model TweedieRegressor\n",
    "            # model = linear_model.TweedieRegressor(power=1, alpha=0.5, link='log')\n",
    "            # Does not work!\n",
    "\n",
    "            # Stochastic Gradient Descent\n",
    "            # model = linear_model.SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n",
    "            # Does not provide good result\n",
    "\n",
    "            # model  = linear_model.Perceptron(tol=1e-3, random_state=0)\n",
    "            # Does not provide good result\n",
    "\n",
    "            # fit the model\n",
    "            model.fit(Xtrain, ytrain)\n",
    "\n",
    "            # Test the model on test data\n",
    "            y_pred = model.predict(Xtest)\n",
    "        else:\n",
    "            y_pred = None\n",
    "            print(\"\\tCAUTION! No Model defined\")\n",
    " \n",
    "    return model, [Xtrain, ytrain], [Xtest, ytest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Evualuate the performance of the model\n",
    "\n",
    "def get_model_performance(model, Xy_train, Xy_test, Xy):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Evaluate the model by computing its R2 score.\n",
    "    INPUT\n",
    "        model is the model to evaluate\n",
    "        Xy_train is a list of train data: [X_train, y_train]\n",
    "        Xy_test is a list of test data: [X_test, y_test]\n",
    "        Xy is a list of with all data: [X, y]\n",
    "    OUTPUT\n",
    "        score (float) in the measure of the model's performance\n",
    "    '''\n",
    "\n",
    "    # Get data sets\n",
    "    X_train, y_train = Xy_train\n",
    "    X_test, y_test = Xy_test\n",
    "    X, y = Xy\n",
    "\n",
    "    # Evaluate this model by gettings metrics with a model by\n",
    "    #  regression\n",
    "    print(\"- Metrics:\")\n",
    "    \n",
    "    # Get the model's score\n",
    "    # Return the coefficient of determination of the prediction\n",
    "    # mdl_score_train = model.score(X_train, y_train)\n",
    "    # print(\"  > Model score (train):\", mdl_score_train)\n",
    "    # mdl_score_test = model.score(X_test, y_test)\n",
    "    # print(\"  > Model score (test):\", mdl_score_test)\n",
    "    # mdl_score = model.score(X, y)\n",
    "    # print(\"  > Model score (all) :\", mdl_score)\n",
    "\n",
    "    # Accuracy_score (https://scikit-learn.org/stable/modules/\n",
    "    # generated/sklearn.metrics.accuracy_score.html)\n",
    "    # Confusion matrix (https://scikit-learn.org/stable/modules/\n",
    "    # generated/sklearn.metrics.confusion_matrix.html)\n",
    "    #  not appropriate for my purpose\n",
    "    # Common pitfalls (https://scikit-learn.org/stable/common_\n",
    "    #  pitfalls.html): mean_sqaured_error and r2_score\n",
    "\n",
    "    # According to https://stackoverflow.com/questions/37367405/\n",
    "    #  python-scikit-learn-cant-handle-mix-of-multiclass-and-continuous\n",
    "    # ... Accuracy score is only for classification problems\n",
    "    # acc_score_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    # acc_score_test = accuracy_score(y_test, model.predict(X_test))\n",
    "    # if debug: print(\"    - acc_score_train:\", acc_score_train)\n",
    "    # if debug: print(\"    - acc_score_test:\", acc_score_test)\n",
    "\n",
    "    # always according to https://stackoverflow.com/questions/37367405/\n",
    "    #  python-scikit-learn-cant-handle-mix-of-multiclass-and-continuous\n",
    "    # For regression problems, use: R2 Score, MSE (Mean Squared Error),\n",
    "    # RMSE (Root Mean Squared Error).\n",
    "    \n",
    "    # r2_score_train = r2_score(y_train, model.predict(X_train))\n",
    "    # r2_score_test = r2_score(y_test, model.predict(X_test))\n",
    "    r2_score_all = r2_score(y, model.predict(X))\n",
    "    # print(\"  > r2 score (train):\", r2_score_train)\n",
    "    # print(\"  > r2 score (test):\", r2_score_test)\n",
    "    # print(\"  > r2 score (all) :\", r2_score_all)\n",
    "\n",
    "    score = r2_score_all\n",
    "    print(\"  > Model's score: \", score)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING -  Indicate the weight of every parameters\n",
    "#  in this model\n",
    "\n",
    "def coef_weights(model, X) -> object:\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "       Returns a dataframe with coefficients of the model\n",
    "        (real and absolute values) sorted in the descending order\n",
    "        of the absolute values\n",
    "    INPUT       \n",
    "       model is the model for which we are looking coefficients\n",
    "       X is the exploratory set of data\n",
    "    OUTPUT\n",
    "      coefs_df : model's coefficients that can be used to understand\n",
    "                 the most influential coefficients in a linear model\n",
    "                 by providing the coefficient estimates along with the\n",
    "                 name of the variable attached to the coefficient.\n",
    "    '''\n",
    "    coefs_df = pd.DataFrame()\n",
    "    # Get name of every column in front  of its coefficients\n",
    "    coefs_df['est_int'] = X.columns\n",
    "    # get coefficients of the linear model\n",
    "    coefs_df['coefs'] = model.coef_\n",
    "    # get absolute value of these coefficients\n",
    "    coefs_df['abs_coefs'] = np.abs(model.coef_)\n",
    "    # Sort coefficient by descending order\n",
    "    coefs_df = coefs_df.sort_values('abs_coefs', ascending=False)\n",
    "\n",
    "    return coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Define, evualute and discuss the model\n",
    "\n",
    "def process_model(df, response, drop_list=[]):\n",
    "    '''\n",
    "    DESCRIPTION\n",
    "        Define, evaluate and discuss a model\n",
    "    INPUT\n",
    "        df is the pandas dataframe with data to build and evaluate the \n",
    "           model;\n",
    "        response (string) is the category / label of the column defined\n",
    "                 as response/target for the model to define;\n",
    "        drop_list (list) is a list of categories / labels of the columns\n",
    "                  that can be removed from the dataframe before \n",
    "                  defining and evaluating the model. It does not modify\n",
    "                  input dataframe. Default = nil (empty list)\n",
    "    OUTPUT\n",
    "        model is the trained model define to predict the response\n",
    "              according to the other labels of the dataframe.\n",
    "        score is the metrics to measure the performance of the model.coef_\n",
    "              closer to 1, better.\n",
    "        coefs is the list of labels of the dataframe used to establish\n",
    "              the model for which a weight (contribution to the model) is\n",
    "              computed (signed and absolute values).\n",
    "    '''\n",
    "    df_mdl = df.copy(deep=True)\n",
    "    print('Modeling...')\n",
    "    # Remove this list of labels\n",
    "    print('- drop')\n",
    "    if len(drop_list) > 0:\n",
    "        df_mdl.drop(columns=drop_list, inplace=True)\n",
    "\n",
    "    # Split into Response y / Exploratory variables X\n",
    "    print(\"- Split into X/y\")\n",
    "    X, y = get_X_y(df_mdl, response)\n",
    "\n",
    "    # modeling\n",
    "    print(\"- Get model\")\n",
    "    model, Xy_train, Xy_test = get_model(X, y)\n",
    "\n",
    "    # Check performance\n",
    "    print(\"- Performance\")\n",
    "    score = get_model_performance(model, Xy_train, Xy_test, [X, y])\n",
    "\n",
    "    # Coefficients of weight\n",
    "    print(\"- Model's coefficients\")\n",
    "    coefs = coef_weights(model, X)\n",
    "    pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "    print(coefs)\n",
    "\n",
    "    return model, score, coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Apply the model\n",
    "\n",
    "# define the target\n",
    "response = 'hosp'\n",
    "drop_list = []\n",
    "# Process the target\n",
    "model, score, coefs = process_model(df_mdl, response, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - refine the dataframe\n",
    "\n",
    "# define the target\n",
    "response = 'hosp'\n",
    "\n",
    "# According to the analysis of the weights of every labels,\n",
    "#  I propose to remove some of them with neglictible weight\n",
    "# < 0.1 in absolute value\n",
    "drop_list2 = ['couv_rappel', 'couv_2_rappel',\n",
    "                'n_dose1', 'n_rappel',\n",
    "                'n_cum_dose1',\n",
    "                'n_complet', 'n_cum_complet',\n",
    "                'n_2_rappel', 'n_3_rappel',\n",
    "                'n_cum_rappel', 'n_cum_2_rappel', 'n_cum_3_rappel',\n",
    "                'n_rappel_biv', 'n_cum_rappel_biv'\n",
    "               ]\n",
    "\n",
    "# Process the target\n",
    "model2, score2, coefs2 = process_model(df_mdl, response, drop_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Apply the model\n",
    "\n",
    "# define the target\n",
    "response = 'TO'\n",
    "drop_list = []\n",
    "# Process the target\n",
    "model3, score3, coefs3 = process_model(df_mdl, response, drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the rate of hospitalization\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the number of people in the population according to\n",
    "#  pop = n_cum_dose1 / couv_dose1\n",
    "df_mdl = divide_df(df_mdl, 'n_cum_dose1', 'couv_dose1', 'pop', description=True)\n",
    "# update the labels dictionary\n",
    "df_dict_labels['pop'] = 'Population'\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING\n",
    "\n",
    "# Compute the rate of hospitalized epople\n",
    "#  tx_hosp = hosp / pop\n",
    "df_mdl = divide_df(df_mdl, 'hosp', 'pop', 'tx_hosp', description=True)\n",
    "# update the labels dictionary\n",
    "df_dict_labels['tx_hosp'] = 'Rate of hospitalization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# DATA PROCESSING - MODELING - Apply the model\n",
    "\n",
    "# define the target\n",
    "response = 'tx_hosp'\n",
    "drop_list = []\n",
    "# Process the target\n",
    "model4, score4, coefs4 = process_model(df_mdl, response, drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the results - Question 5:\n",
    "\n",
    "   The model to predict COVID-19 positive cases per week gives rather good\n",
    "   result\n",
    "    with a global score of 0.88.\n",
    "   Analysis of the contribution of every parameters shows that the result is\n",
    "    mainly linked with the Occupancy rate (TO) which is actuall rather \n",
    "\tconsequence of the prediction.\n",
    "   It shows a high level a correlation between the number of positive cases\n",
    "    and the amount of work in the hospitals despite the vaccination campaign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDINGS\n",
    "\n",
    "About 6% of data were removed from the initial data set due to missing \n",
    " values. It concerns mainly first set of data at the beginning of the data \n",
    " collection. Indeed, method, measure, collection and presentation of data \n",
    " change in time before getting the current format of data.\n",
    " Nevertheless, we benefit from a huge amount of valid data for the analysis.\n",
    " \n",
    "Computation of some rates raised the problem of division by 0. While taking \n",
    " about group of people and with the will of showing result for every \n",
    " department, I replace 0 by 1 when necessary.\n",
    " \n",
    "I also face infinite values; then I replaced with infinite values by maximum\n",
    " numerical values of the serie, in order to be reasonable realistic and show\n",
    " result.\n",
    "\n",
    "Although testing several type of models, I kept the Linear Regression which\n",
    " give rather good results with an implementation rather simple. \n",
    "I wonder if some other type of models could be more suitable in this case.\n",
    "\n",
    "Display of maps and plots from the Notebook was not easy due to difficulties\n",
    "of installation of additional libraries and other things required to make it \n",
    "run correctly; all these installations were not possible on my profressional\n",
    "laptop to safety policy of my company.\n",
    "Nevertheless, display of these graph is possible with the python code."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0600588c3b5f4418cbe7b5ebc6825b479f3bc010269d8b60d75058cdd010adfe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
